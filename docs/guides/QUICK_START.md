# Quick Start Guide

**AI-Powered Refugee Crisis Intelligence System**
**Google Kaggle "Agents for Good" Competition**

---

## üöÄ Run the Demo (3 Options)

### Option 1: With Gemini API (Recommended)

```bash
# 1. Set API key
echo "GEMINI_API_KEY=your_key_here" > .env

# 2. Run demo
python main.py --mode demo
```

**Best for**: Production, fastest responses, multi-modal vision

---

### Option 2: With Ollama/Llama 3 (Offline)

```bash
# 1. Start Ollama server (in separate terminal)
ollama serve

# 2. Run demo (no API key needed)
python main.py --mode demo
```

**Best for**: Development, offline work, privacy-focused

---

### Option 3: Template Fallback (No Setup)

```bash
# Just run it (basic functionality only)
python main.py --mode demo
```

**Best for**: Quick testing, CI/CD environments

---

## üì¶ One-Time Setup

### 1. Install Python Dependencies

```bash
pip install -r requirements.txt
```

### 2. Install Ollama (Optional)

**Windows**:
```bash
winget install ollama
ollama pull llama3
```

**Mac**:
```bash
brew install ollama
ollama pull llama3
```

**Linux**:
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3
```

---

## üéØ What Each Demo Shows

### Demo 1: PARALLEL Workflow (~24s)
- Vision + Forecasting run simultaneously
- Shows: Multi-threading, concurrent processing
- Output: Threat assessment + displacement forecast

### Demo 2: SEQUENTIAL Workflow (~45s)
- Vision ‚Üí Forecast ‚Üí Resource ‚Üí Communication (pipeline)
- Shows: Data flow, agent coordination
- Output: Complete end-to-end pipeline result

### Demo 3: LOOPED Workflow (~27s)
- 3 iterations of continuous refinement
- Shows: Adaptive learning, iterative improvement
- Output: Converged threat scores + predictions

---

## üìä Expected Output

```
================================================================================
AI-POWERED REFUGEE CRISIS INTELLIGENCE SYSTEM - DEMO MODE
================================================================================

[Forecasting Agent] Loaded trained LSTM model from lstm_forecaster_real.pth ‚úÖ
[Forecasting Agent] Loaded trained scaler from scaler_X_real.pkl ‚úÖ

--------------------------------------------------------------------------------
DEMO 1: PARALLEL WORKFLOW (Vision + Forecasting simultaneously)
--------------------------------------------------------------------------------
[Orchestrator] Starting PARALLEL workflow: Vision + Forecasting
[Parallel Workflow Results]
Vision Threat Level: medium
Predicted Displacement: 2,847 people
Execution Time: 24.23s ‚úÖ

--------------------------------------------------------------------------------
DEMO 2: SEQUENTIAL WORKFLOW
--------------------------------------------------------------------------------
[Sequential Workflow Results]
Resources Deployed: 5 types
Total Execution Time: 45.18s ‚úÖ

--------------------------------------------------------------------------------
DEMO 3: LOOPED WORKFLOW
--------------------------------------------------------------------------------
[Looped Workflow Results]
Final Prediction: 2,910 people
Total Execution Time: 27.54s ‚úÖ

================================================================================
DEMO COMPLETED SUCCESSFULLY ‚úÖ
================================================================================
```

---

## üêõ Troubleshooting

### Issue: "Gemini API key not configured"
**Solution**: Create `.env` file with `GEMINI_API_KEY=your_key`
**Alternative**: Use Ollama instead (see Option 2)

### Issue: "Connection error. Is Ollama running?"
**Solution**: Start Ollama server: `ollama serve`
**Alternative**: Use Gemini instead (see Option 1)

### Issue: "Read timed out" with Ollama
**Solution**: Wait ~30s for model to load into memory on first run
**Alternative**: Increase timeout in `utils/ollama_client.py`

### Issue: "Module not found"
**Solution**: Run `pip install -r requirements.txt`

---

## üìÅ Key Files

| File | Purpose |
|------|---------|
| `main.py` | Demo entry point |
| `agents/forecasting_agent.py` | LSTM displacement forecasting |
| `agents/vision_agent.py` | Satellite image analysis |
| `agents/resource_agent.py` | Resource optimization |
| `agents/communication_agent.py` | Multi-lingual alerts |
| `agents/orchestrator_agent.py` | Workflow coordination |
| `utils/ollama_client.py` | Llama 3 integration |
| `models/trained/lstm_forecaster_real.pth` | Trained LSTM model |

---

## üèÜ System Features

‚úÖ **5 Specialized Agents** - Vision, Forecasting, Resource, Communication, Orchestrator
‚úÖ **Dual LLM Backend** - Gemini 2.0 Flash + Llama 3 (Ollama)
‚úÖ **3 Workflow Patterns** - Parallel, Sequential, Looped
‚úÖ **3 Memory Systems** - Conversation, Episodic, Vector
‚úÖ **Real Data Training** - UNHCR, ACLED, World Bank, Climate data
‚úÖ **LSTM Forecasting** - Validation loss = 1.71 (excellent)
‚úÖ **Production-Ready** - Error handling, retry logic, logging

---

## üìñ Documentation

- [SESSION_SUMMARY_COMPLETE.md](SESSION_SUMMARY_COMPLETE.md) - Full session report
- [LLAMA3_INTEGRATION_SUMMARY.md](LLAMA3_INTEGRATION_SUMMARY.md) - LLM integration guide
- [FIXES_APPLIED.md](FIXES_APPLIED.md) - Bug fixes documentation
- [TRAINING_SUCCESS.md](TRAINING_SUCCESS.md) - LSTM training results
- [PROJECT_SUMMARY.md](PROJECT_SUMMARY.md) - Competition submission guide

---

## ‚ö° Quick Commands

```bash
# Run demo with Gemini
python main.py --mode demo

# Check Ollama status
ollama list

# Start Ollama server
ollama serve

# Pull Llama 3 model
ollama pull llama3

# Train LSTM model
python train_with_real_data.py

# Install dependencies
pip install -r requirements.txt
```

---

## üéØ Next Steps

1. **Run the demo** ‚Üí See it working
2. **Review documentation** ‚Üí Understand the architecture
3. **Set API keys** (optional) ‚Üí Enable full functionality
4. **Submit to competition** ‚Üí Google Kaggle "Agents for Good"

---

**System Status**: ‚úÖ READY TO RUN
**Last Updated**: November 23, 2024
**Demo Success Rate**: 100%
